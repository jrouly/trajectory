{"name":"Trajectory","tagline":"Content analysis of university course descriptions.","body":"# Trajectory\r\n\r\n[![Join the chat at https://gitter.im/jrouly/trajectory](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jrouly/trajectory?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\r\n\r\nTrajectory is a working title for my CS 390 undergraduate research project. I am taking university course description data and applying LDA topic modeling.\r\n\r\n## Requirements\r\n\r\nThe basic requirements are Java JDK 7 or higher, Python 3.0 or higher, `virtualenv`, and `maven`. Support for the database layer requires system copies of MySQL, PostGres, SQLite, or similar software. Support for the visualization layer requires a proxy web server (eg. Apache, Nginx).\r\n\r\n\r\n## Setup\r\n\r\nNote that this project contains an unholy combination of Bash scripts, Python tools, and Java code. Proceed with setup carefully.\r\n\r\nBegin by exporting the `$TRJ_HOME` path variable.\r\n\r\n    $ git clone http://github.com/jrouly/trajectory\r\n    $ cd trajectory\r\n    $ export TRJ_HOME=$(pwd)\r\n\r\nInstall Python dependencies by calling the `bin/util/pysetup` script. Java code will be compiled on demand.\r\n\r\nTo specify or change the database URI and scheme, modify the `config.py` file. Specifically, look for `DATABASE_URI`. It defaults to a SQLite file named `data.db`.\r\n\r\n### Visualization server\r\n\r\n#### Sample nginx configuration\r\n\r\n    server {\r\n        listen 80;\r\n        location ^~ /static/  {\r\n            root /TRJ_HOME/src/main/resources/web/static;\r\n        }\r\n\r\n        location / {\r\n            proxy_pass         http://localhost;\r\n            proxy_redirect     off;\r\n            proxy_set_header   Host $host;\r\n            proxy_set_header   X-Real-IP $remote_addr;\r\n            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\r\n            proxy_set_header   X-Forwarded-Host $server_name;\r\n        }\r\n    }\r\n\r\n## Use\r\n\r\n### Download data from a prebuilt target\r\n\r\n    $ bin/scrape download [-h] {targets}\r\n\r\n### Export downloaded data to disk\r\n\r\n    $ bin/scrape export [-h] [--data-directory <directory>]\r\n                  [--departments <departments>] [--cs]\r\n\r\nThis exports data in a format that can be read in by the `Learn` module. The data directory will default to `data/`. You can selectively filter subjects exported using the `--departments` flag.\r\n\r\n### Run Topic Modeling\r\n\r\n    $ bin/learn -in <path> -out <path> [-iterations <n>] [-debug]\r\n                  [-threads <n>] [-topics <n>] [-words <n>]\r\n                  [-alpha <alpha>] [-beta <beta>]\r\n\r\nThe `-in` parameter must be an export location from the `Scrape` module. Results will be stored within a timestamped subdirectory of the `-out` directory. All other parameters are optional.\r\n\r\n### Import topic modeling to database\r\n\r\n    $ bin/scrape import-results [-h] --topic-file <file> --course-file <file>\r\n                  [--alpha <alpha>] [--beta <beta>] [--iterations <iterations>]\r\n\r\nRead the results of the `Learn` module (inferred topics) back into the database and pair with existing course data. Multiple imports will simply add `ResultSet`s to the existing database.\r\n\r\n### Run visualizations server\r\n\r\n    $ bin/web\r\n\r\nActivate the visualization server. See `gunicorn.py` for configuration settings. Notice that the PID and log files are stored in the `TRJ_HOME`.\r\n\r\n# ToDo\r\n\r\n1. Identify necessary RegExes to pull prereqs out of each existing engine.\r\n2. Extract ACM descriptions to an engine.\r\n3. Run test suite varying alpha/beta/topics parameters.\r\n4. Implement the \"timeline\" visualization tool (tracking prerequisites).\r\n5. Implement the \"graph\" visualization tool to visualize topics and courses.\r\n6. Refactor configuration objects as a module.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}