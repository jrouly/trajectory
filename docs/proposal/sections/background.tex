\section{Background on \acf{lda}}
\label{sec:background}

%------------------------------------------------

Topic modeling, a form of latent variable modeling, is an unsupervised machine learning method which attempts to recreate the distribution of so-called ``topics'' an author used to generate a corpus of documents.
The term topic is used to describe a frequency distribution of terms within a vocabulary.
In this use, a topic can be understood to represent an academic concept covered within the context of a course.
The topics discovered in a corpus can be used to categorize documents and provide structure to an otherwise unknown dataset.

%------------------------------------------------

\acf{lda} is a specific type of topic modeling which assumes that a mix of multiple topics exist within a single document in some proportion (ie.\ were used to generate that document). \citep{Blei2003}
\ac{lda} assumes a generative process where, for each word in the document, the algorithm selects a distribution over topics, selects a topic, and then selects a vocabulary term. \citep{Blei2003}
By picking a distribution over topics, multiple possible topics can be blended into a single document.
Reversing this generative process is significantly more difficult because the topic distributions are unknown; this is what the ``hidden model'' or ``latent model'' refers to.

%------------------------------------------------

\ac{lda} can best be understood through its generative process.
Given the set of distributions as input, generating the corpus topics is a probabilistic process.
Taking the variables $\theta_{d,k}$ (topic proportion for topic $k$ in document $d$), $\beta_{1:k}$ (topic $k$), $z_{d,n}$ (topic assignment for word $n$ in document $d$), and $w_{d,n}$ (the $n^{th}$ word in document $d$), \ac{lda} calculates the posterior probability in Equation~\ref{eq:posterior}. \citep{Blei2012}

\begin{equation}
p(\beta_{1:K}, \theta_{1:D},z_{1:D} | w_{1:D}) = \frac{\beta_{1:K},
\theta_{1:D},z_{1:D}, w_{1:D}}{w_{1:D}}
\label{eq:posterior}
\end{equation}

%------------------------------------------------

\noindent
Using a variety of probabilistic methods to calculate or approximate the denominator (ie.\ evidence), \ac{lda} results in a usable set of vocabulary frequency distributions or topics.
Specifically, Gibbs Sampling, a variety of Bayesian Inference, is used to approximate the \ac{lda} posterior probability. \cite{Blei2003}

%------------------------------------------------

