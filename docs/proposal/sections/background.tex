Topic modeling, a form of latent variable modeling, is an unsupervised
machine learning method which attempts to recreate the distribution of
so-called ``topics'' an author used to generate a corpus of documents. In
this case, a topic is a frequency distribution of terms within a
vocabulary. The topics discovered in a corpus can be used to categorize
documents and provide structure to an otherwise unknown dataset.

\ac{lda} is a specific type of topic modeling which assumes that multiple
topics exist within a single document (ie.\ were used to generate that
document). \citep{Blei2003} \ac{lda} assumes a generative process where,
for each word in the document, it selects a distribution over topics,
selects a topic, and then selects a vocabulary term. \citep{Blei2003} By
picking a distribution over topics, multiple possible topics can be blended
into a single document. Reversing this generative process is significantly
more difficult because the topic distributions are unknown. This is what
the ``hidden model'' or ``latent model'' refers to.

\ac{lda} can best be understood through its generative process. Given the
set of distributions as input, generating the corpus topics is a
probabilistic process. Taking the variables $\theta_{d,k}$ (topic
proportion for topic $k$ in document $d$), $\beta_{1:k}$ (topic $k$),
$z_{d,n}$ (topic assignment for word $n$ in document $d$), and
$w_{d,n}$ (the $n^{th}$ word in document $d$), \ac{lda} calculates the
posterior probability in Equation~\ref{eq:posterior}. \citep{Blei2012}

\begin{equation}
p(\beta_{1:K}, \theta_{1:D},z_{1:D} | w_{1:D}) = \frac{\beta_{1:K},
\theta_{1:D},z_{1:D}, w_{1:D}}{w_{1:D}}
\label{eq:posterior}
\end{equation}

\noindent
Using a variety of probabilistic methods to calculate or approximate the
denominator (ie.\ evidence) \ac{lda} results in a usable set of vocabulary
frequency distributions or topics. Specifically, Gibbs Sampling, a variety
of Bayesian Inference, is used to approximate the \ac{lda} posterior
probability.\cite{Blei2003}

Additional unsupervised machine learning tools can be applied to the same
problem. Simple clustering algorithms (eg. K-Means) when given the same bag
of words corpus representation as input act to identify groupings of
similar documents according to their term frequency vector Euclidean
distance. \cite{lloyd1982} Additional, similar clustering algorithms can be
applied in a similar manner.
